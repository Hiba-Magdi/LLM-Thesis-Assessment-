{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiba-Magdi/LLM-Thesis-Assessment-/blob/main/Copy_of_LLMthesis_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsZeSSSWbrw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e744ac-8981-4759-a688-33063c1448ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.32)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.3.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.9\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Collecting frontend\n",
            "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
            "Collecting starlette>=0.12.0 (from frontend)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting uvicorn>=0.7.1 (from frontend)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from frontend) (2.2.0)\n",
            "Collecting aiofiles (from frontend)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette>=0.12.0->frontend) (3.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.7.1->frontend) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.1)\n",
            "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: uvicorn, aiofiles, starlette, frontend\n",
            "Successfully installed aiofiles-24.1.0 frontend-0.0.3 starlette-0.45.3 uvicorn-0.34.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "Collecting pinecone[grpc]\n",
            "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2024.12.14)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (1.66.0)\n",
            "Requirement already satisfied: grpcio>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (1.70.0)\n",
            "Collecting lz4>=3.1.3 (from pinecone[grpc])\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone[grpc])\n",
            "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone[grpc])\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=4.25 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.25.6)\n",
            "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone[grpc])\n",
            "  Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
            "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, lz4, protoc-gen-openapiv2, pinecone-plugin-inference, pinecone\n",
            "Successfully installed lz4-4.4.3 pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 protoc-gen-openapiv2-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip -qq install streamlit google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "!pip install pyngrok\n",
        "!pip install langchain-google-genai\n",
        "!pip -qq install pymupdf4llm\n",
        "!pip install python-docx\n",
        "!pip install python-dotenv\n",
        "# RAG Depenencies\n",
        "!pip install PyMuPDF\n",
        "!pip install frontend\n",
        "!pip install tiktoken\n",
        "!pip install \"pinecone[grpc]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from google import genai  # Using GenerativeModel directly\n",
        "import pymupdf4llm\n",
        "from docx import Document\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from dotenv import load_dotenv\n",
        "from google.genai.types import (GenerateContentConfig)\n",
        "\n",
        "\n",
        "# RAG Imports\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import json\n",
        "import tiktoken\n",
        "import fitz\n",
        "\n",
        "# --- Input Preprocessing Class ---\n",
        "\n",
        "class ThesisInputProcessor:\n",
        "    def __init__(self, temp_dir=\"temp\"):\n",
        "        self.temp_dir = temp_dir\n",
        "        os.makedirs(self.temp_dir, exist_ok=True)\n",
        "\n",
        "    def convert_thesis_to_md(self, uploaded_file):\n",
        "\n",
        "        \"\"\"Converts a PDF thesis to Markdown.\"\"\"\n",
        "\n",
        "        file_path = os.path.join(self.temp_dir, uploaded_file.name)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "        return pymupdf4llm.to_markdown(file_path)\n",
        "\n",
        "    def extract_description_text(self, uploaded_description):\n",
        "\n",
        "        \"\"\"Extracts text from a Word document.\"\"\"\n",
        "\n",
        "        doc = Document(uploaded_description)\n",
        "        return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "    def load_evaluation_criteria(self, file_path):\n",
        "        \"\"\"\n",
        "        Loads evaluation criteria from a JSON file.\n",
        "        \"\"\"\n",
        "        with open(file_path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "\n",
        "class PineconeVectorDB:\n",
        "    def __init__(self, api_key, environment, index_name):\n",
        "        self.pinecone = Pinecone(api_key=api_key)\n",
        "        self.index_name = index_name\n",
        "        if index_name not in self.pinecone.list_indexes().names():\n",
        "            self.pinecone.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1024,\n",
        "                metric='euclidean',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'\n",
        "                )\n",
        "            )\n",
        "        self.index = self.pinecone.Index(index_name)\n",
        "\n",
        "    def embed_text(self, text):\n",
        "        \"\"\"\n",
        "        Embedds text using the Pinecone API.\n",
        "        \"\"\"\n",
        "        embedding = self.pinecone.inference.embed(\n",
        "            model=\"multilingual-e5-large\",\n",
        "            inputs=[text],\n",
        "            parameters={\"input_type\": \"passage\", \"truncate\": \"END\"},\n",
        "        )\n",
        "        return embedding[0]\n",
        "\n",
        "    def chunk_text_by_tokens(self, text, chunk_size, encoding_name=\"cl100k_base\"):\n",
        "        \"\"\"\n",
        "        Splits the text into chunks based on the number of tokens.\n",
        "        \"\"\"\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        tokens = encoding.encode(text)\n",
        "        return [encoding.decode(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    def extract_text(self, pdf_path):\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "\n",
        "    def get_number_of_vectors(self):\n",
        "        return self.index.describe_index_stats()['total_vector_count']\n",
        "\n",
        "    def upsert_pdf(self, pdf_file_path):\n",
        "        try:\n",
        "            pdf_text = self.extract_text(pdf_file_path)\n",
        "            chunks = self.chunk_text_by_tokens(text=pdf_text, chunk_size=500)\n",
        "            vectors = []\n",
        "            i=self.get_number_of_vectors()\n",
        "            for chunk in chunks:\n",
        "                text = chunk\n",
        "                vector = self.embed_text(text).values\n",
        "                vectors.append({\"id\": f\"vec{i}\",\n",
        "                                \"values\": vector,\n",
        "                                \"metadata\":{\n",
        "                                    \"text\" : text,\n",
        "                                    \"source\": \"pdf_file\",\n",
        "                                }\n",
        "                            })\n",
        "                i+=1\n",
        "            self.index.upsert(vectors=vectors)\n",
        "        except Exception as e:\n",
        "            print(f\"Error inserting data: {e}\")\n",
        "\n",
        "    def inject_vectors(self, vectors):\n",
        "        \"\"\"\n",
        "        Injects a list of vectors into the Pinecone index.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.index.upsert(vectors=vectors)\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting vectors: {e}\")\n",
        "\n",
        "    def fetch_vectors(self, query, top_k=10):\n",
        "        \"\"\"\n",
        "        Fetches the top_k most similar vectors to the given query vector.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            query_embedding = query\n",
        "            results = self.index.query(\n",
        "                vector=query_embedding,\n",
        "                top_k=top_k,\n",
        "                include_metadata=True\n",
        "            )\n",
        "            return results.matches\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching vectors: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "\n",
        "class GoogleDriveUploader:\n",
        "    \"\"\"Handles uploading files to Google Drive.\"\"\"\n",
        "    def __init__(self, api_credentials, folder_id):\n",
        "        self.creds = service_account.Credentials.from_service_account_info(api_credentials)\n",
        "        self.drive_service = build('drive', 'v3', credentials=self.creds)\n",
        "        self.folder_id = folder_id\n",
        "        self.temp_dir = \"temp\"  # Add TEMP_DIR as an attribute\n",
        "\n",
        "    def load_api_credentials(file_path):\n",
        "      \"\"\"\n",
        "       Loads Google Drive API credentials from a json file .\n",
        "      \"\"\"\n",
        "      with open(file_path, \"r\") as f:\n",
        "\n",
        "         return json.load(f)\n",
        "\n",
        "    def upload_file(self, file_path):\n",
        "        file_metadata = {\n",
        "            'name': os.path.basename(file_path),\n",
        "            'parents': [self.folder_id],\n",
        "        }\n",
        "        media = MediaFileUpload(file_path, mimetype='text/markdown')\n",
        "        uploaded_file = self.drive_service.files().create(\n",
        "            body=file_metadata,\n",
        "            media_body=media,\n",
        "            fields='id, name'\n",
        "        ).execute()\n",
        "        return uploaded_file\n",
        "\n",
        "    def upload_evaluation(self, filename, evaluation_text):\n",
        "        \"\"\"Uploads the evaluation result to Google Drive.\"\"\"\n",
        "        result_file_path = os.path.join(self.temp_dir, f\"{filename.replace('.pdf', '')}_evaluation.txt\")\n",
        "        with open(result_file_path, \"w\") as result_file:\n",
        "            result_file.write(evaluation_text)\n",
        "        try:\n",
        "            uploaded_gdrive_file = self.upload_file(result_file_path) # Use the class's upload_file method\n",
        "            st.success(f\"Uploaded evaluation for '{filename}' to Google Drive successfully!\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error uploading evaluation for '{filename}' to Google Drive: {e}\")\n",
        "\n",
        "#Retrival context\n",
        "def get_context(text, vdb):\n",
        "    chunks = vdb.chunk_text_by_tokens(text, chunk_size=500)\n",
        "    queries = []\n",
        "    for chunk in chunks:\n",
        "        query = vdb.embed_text(chunk)\n",
        "        queries.append(query)\n",
        "    context='### Context Begins here:\\n'\n",
        "    for query in queries:\n",
        "        results = vdb.fetch_vectors(query.values)\n",
        "        for result in results:\n",
        "            context += result.metadata['text'] + '\\n'\n",
        "    context+=\"### Context Ends here\\n\"\n",
        "    return context\n",
        "\n",
        "def handle_rag_context(uploaded_criteria_files, vdb):\n",
        "    if uploaded_criteria_files:\n",
        "        for criteria_file in uploaded_criteria_files:\n",
        "            criteria_file_path = os.path.join(TEMP_DIR, criteria_file.name)\n",
        "            if criteria_file.name not in st.session_state.get('criteria_contexts', {}): # Use get to avoid KeyError\n",
        "                with open(criteria_file_path, \"wb\") as f_criteria:\n",
        "                    f_criteria.write(criteria_file.getbuffer())\n",
        "                vdb.upsert_pdf(criteria_file_path)\n",
        "                criteria_text = json.dumps(evaluation_criteria)\n",
        "                context = get_context(criteria_text, vdb)\n",
        "                st.session_state['criteria_contexts'][criteria_file.name] = context  # Store in session state\n",
        "            else:\n",
        "                context = st.session_state['criteria_contexts'][criteria_file.name]\n",
        "        return context # Return context after processing all files.\n",
        "    else:\n",
        "        criteria_text = json.dumps(evaluation_criteria)\n",
        "        context = get_context(criteria_text, vdb)\n",
        "        return context\n",
        "\n",
        "def generate_evaluation_prompt_direct(thesis_name, criteria, md_text, description=\"\", context=\"\"):\n",
        "    prompt = f\"\"\"You are an expert in academic research and evaluation. Evaluate the following thesis titled \"{thesis_name}\" based on the criteria listed below:\n",
        "\n",
        "Evaluation Criteria:\n",
        "{criteria}\n",
        "\n",
        "Thesis Description:\n",
        "{description}\n",
        "\n",
        "Thesis Content:\n",
        "{md_text}\n",
        "\n",
        " context :\n",
        "\n",
        "{context}\n",
        "\n",
        "# Instructions:\n",
        "\n",
        "1. Analyse the thesis provided. If something other than an academic paper was\n",
        "provided if it does not contain the following chapters : Abstract, Literture review , Methodology , do NOT say anything else , and say ONLY the following:\n",
        " Do not analyze and provide any Feedback , your response MUST BE: \"This document does not appear to be a thesis. Please make sure that you\n",
        "submitted the correct paper\"\n",
        "\n",
        "\n",
        "2. While evaluating the thesis, The following questions should be answered based on the corresponding 'Evaluation Criteria' category:\n",
        "\n",
        "Abstract:\n",
        "    Does the abstract provide a concise summary of the research activities?\n",
        "    Does the abstract cover all essential aspects of the research, as described in the evaluation criteria for the 'Abstract' category?\n",
        "\n",
        "Primary/Secondary Research:\n",
        "    What type of research is primarily used (tertiary, secondary, primary)?\n",
        "    Are the sources credible and appropriately recognized?\n",
        "    Does the research approach align with the descriptions in the evaluation criteria for 'Primary/Secondary Research'?\n",
        "\n",
        "Research aim, objectives and questions:\n",
        "    Are the research aim, objectives, and questions clearly defined?\n",
        "    Is there a clear alignment between the aim, objectives, and research questions?\n",
        "    Does the clarity and alignment match the descriptions in the 'Research aim, objectives and questions' evaluation criteria?\n",
        "\n",
        "Research and Project Method (/ology) - the theory:\n",
        "    Are the arguments well-formed and grounded in rationality?\n",
        "    Are conclusions and solutions justified and supported by concepts and theories?\n",
        "    Does the theoretical methodology align with the descriptions in the 'Research and Project Method (/ology) - the theory' evaluation criteria?\n",
        "\n",
        "Research and Project Method (/ology) - the practical:\n",
        "   Is the practical work relevant, creative, and well-documented?\n",
        "    Is there evidence of creativity and effort in the practical work?\n",
        "    Does the practical methodology and documentation meet the descriptions in the 'Research and Project Method (/ology) - the practical' evaluation criteria?\n",
        "\n",
        "Result, Discussion and Conclusion:\n",
        "    Does the thesis present a professional image and contribute to the field of computer science?\n",
        "    Are the research questions answered by the results and discussion?\n",
        "    Does the contribution and impact of the results align with the descriptions in the 'Result, Discussion and Conclusion' evaluation criteria?\n",
        "\n",
        "Thesis Structure:\n",
        "    Is the thesis well-structured with clear divisions into chapters, sections, etc.?\n",
        "    Are tables of contents and figures readable and accessible?\n",
        "    Does the thesis structure align with the descriptions in the 'Thesis Structure' evaluation criteria?\n",
        "\n",
        "\n",
        "For each category listed below:\n",
        "1.  Answer the questions provided above for each corresponding category.\n",
        "2. provide a score (1-10)\n",
        "3. justify your score based on the provided score levels descriptions\n",
        "4. identify and list the strengths based on your answers to the questions and the thesis text.\n",
        "5. identify and list the weaknesses based on your answers to the questions and the thesis text. **Provide specific examples from the thesis where each weakness is observed.**\n",
        "6. Provide specific and actionable suggestions for improvement. **For each suggestion, provide an example of how the thesis could be improved.**\n",
        "7. calculate the \"Contribution to Final Rating\" for each category using the formula [Score x Weight]\n",
        "8. Final Score: [Just add all section's contributions to find the overall score out of 60]\n",
        "\n",
        "\n",
        "Answer ALL of the previous requirements in a well-written, formal and comprehensive summary for each category.\n",
        "\n",
        "Write your summarization in the following format:\n",
        "\n",
        "**Abstract**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to the Abstract based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to the Abstract based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to the Abstract. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Primary/Secondary Research**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Primary/Secondary Research based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Primary/Secondary Research based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Primary/Secondary Research. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Research aim, objectives and questions**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Research aim, objectives and questions based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Research aim, objectives and questions based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Research aim, objectives and questions. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Research and Project Method (/ology) - the theory**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Research and Project Method (/ology) - the theory based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Research and Project Method (/ology) - the theory based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Research and Project Method (/ology) - the theory. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Research and Project Method (/ology) - the practical**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Research and Project Method (/ology) - the practical based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Research and Project Method (/ology) - the practical based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Research and Project Method (/ology) - the practical. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Result, Discussion and Conclusion**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Result, Discussion and Conclusion based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Result, Discussion and Conclusion based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Result, Discussion and Conclusion. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Thesis Structure**:\n",
        "\n",
        "**Score**: [Score 1-10]\n",
        "\n",
        "**Justification**: [Justification based on score level descriptions]\n",
        "\n",
        "**Strengths**:\n",
        "1. [Identify and list the strengths related to Thesis Structure based on your answers to the questions and the thesis text.]\n",
        "2. ...\n",
        "\n",
        "**Weaknesses**:\n",
        "1. [Identify and list the weaknesses related to Thesis Structure based on your answers to the questions and the thesis text. **Example from thesis:** ...]\n",
        "2. ...\n",
        "\n",
        "**Suggestions**:\n",
        "1. [Provide specific and actionable suggestions for improvement related to Thesis Structure. **Example Improvement:** ...]\n",
        "2. ...\n",
        "\n",
        "**Contribution to Final Rating**: [Score x Weight]\n",
        "\n",
        "**Overall Evaluation**:\n",
        "**Final Score**:  [Sum of all Contribution to Final Rating scores out of 60]\n",
        "**General Feedback**:\n",
        "[Summarize the overall performance of the thesis, highlighting key strengths and areas for improvement.]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def evaluate_thesis(thesis_name, criteria, md_text, description=\"\", context=\"\"):\n",
        "    # Generate the evaluation prompt based on input arguments\n",
        "    evaluation_prompt = generate_evaluation_prompt_direct(thesis_name, criteria, md_text, description, context)\n",
        "\n",
        "    # Call the Gemini API to generate the evaluation\n",
        "    gemini_config = GenerateContentConfig(\n",
        "temperature=0.0)\n",
        "    response = client.models.generate_content(model='gemini-2.0-flash-exp', contents=evaluation_prompt,config=gemini_config)\n",
        "\n",
        "\n",
        "    # Extract and return the response content\n",
        "    return response.text\n",
        "\n",
        "def evaluate_uploaded_thesis(uploaded_file, uploaded_description, vdb, drive_uploader,input_processor, use_rag, uploaded_criteria_files):\n",
        "    \"\"\"Evaluates a single uploaded thesis file.\"\"\"\n",
        "    with st.spinner(\"Evaluating...\"):\n",
        "        thesis_description_text = \"\"\n",
        "        if uploaded_description:\n",
        "            try:\n",
        "                thesis_description_text = input_processor.extract_description_text(uploaded_description)\n",
        "            except Exception as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "        md_text = input_processor.convert_thesis_to_md(uploaded_file)\n",
        "        thesis_name = uploaded_file.name.replace(\".pdf\", \"\")\n",
        "\n",
        "        if use_rag:\n",
        "            context = handle_rag_context(uploaded_criteria_files, vdb) # New function (see below)\n",
        "        else:\n",
        "            context = \"\"\n",
        "\n",
        "        evaluation_result = evaluate_thesis(thesis_name, evaluation_criteria, md_text, thesis_description_text, context)\n",
        "\n",
        "        st.write(f\"### Evaluation Result for: {uploaded_file.name}\")\n",
        "        st.markdown(evaluation_result)\n",
        "\n",
        "        return evaluation_result  # Return the result for caching\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "\n",
        "#  Access the variables\n",
        "load_dotenv(\"keys.env.txt\")\n",
        "\n",
        "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "pinecone_environment = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
        "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
        "GOOGLE_DRIVE_FOLDER_ID = os.getenv(\"GOOGLE_DRIVE_FOLDER_ID\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "input_processor = ThesisInputProcessor()\n",
        "\n",
        "GOOGLE_API_CREDENTIALS = GoogleDriveUploader.load_api_credentials(\"google_api_cridentials.json\")\n",
        "evaluation_criteria = input_processor.load_evaluation_criteria(\"Rubric.json\")\n",
        "\n",
        "\n",
        "# --- Initialize Components ---\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "drive_uploader = GoogleDriveUploader(GOOGLE_API_CREDENTIALS, GOOGLE_DRIVE_FOLDER_ID)\n",
        "vdb = PineconeVectorDB(pinecone_api_key, pinecone_environment, pinecone_index_name)\n",
        "\n",
        "\n",
        "\n",
        "# --- Streamlit App ---\n",
        "st.title(\"Thesis Evaluation Assistant\")\n",
        "st.write(\"\"\"\n",
        "Upload your thesis (PDF format) and optionally a description (Word document), and I'll evaluate it based on predefined criteria using a direct API call.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "if 'interaction_history' not in st.session_state:\n",
        "    st.session_state['interaction_history'] = []\n",
        "\n",
        "# --- Add RAG toggle to SideBar\n",
        "use_rag = st.sidebar.toggle(\n",
        "    label=\"Use RAG\",\n",
        "    value=False,\n",
        "    help=\"Enable or disable the RAG functionality.\"\n",
        ")\n",
        "use_rag_state = \"enabled\" if use_rag else \"disabled\"\n",
        "st.sidebar.write(f\"Use RAG is {use_rag_state}.\")\n",
        "\n",
        "# --- Add Previous Interactions to SideBar ---\n",
        "with st.sidebar:\n",
        "    # Adding a divider\n",
        "    st.divider()\n",
        "    st.header(\"Previous Evaluations\")\n",
        "    if st.session_state['interaction_history']:\n",
        "        for i, interaction in enumerate(reversed(st.session_state['interaction_history'])):\n",
        "            col1, col2 = st.columns([0.8, 0.2])\n",
        "            with col1:\n",
        "                if col1.button(f\"View: {interaction['filename']}\", key=f\"interaction_{i}\"):\n",
        "                    st.session_state['selected_interaction'] = interaction['evaluation']\n",
        "                    st.session_state['selected_filename'] = interaction['filename']\n",
        "            with col2:\n",
        "                if col2.button(\"X\", key=f\"clear_{i}\",use_container_width=True ):\n",
        "                    st.session_state['interaction_history'].pop(len(st.session_state['interaction_history']) - 1 - i)\n",
        "                    st.rerun()\n",
        "    else:\n",
        "        st.write(\"No previous evaluations yet.\")\n",
        "\n",
        "# --- File Upload and Processing ---\n",
        "st.header(\"Upload Files\")\n",
        "\n",
        "uploaded_criteria_files = None\n",
        "if use_rag:\n",
        "    uploaded_criteria_files = st.file_uploader(\"Upload your criteria files (PDF format)\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "uploaded_description = st.file_uploader(\"Upload thesis description (Word format, optional)\", type=[\"docx\"])\n",
        "uploaded_files = st.file_uploader(\"Upload your thesis files (PDF format)\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "\n",
        "\n",
        "if uploaded_files:\n",
        "    for uploaded_file in uploaded_files:\n",
        "        existing_evaluation = next(\n",
        "            (interaction['evaluation'] for interaction in st.session_state['interaction_history']\n",
        "             if interaction['filename'] == uploaded_file.name),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if existing_evaluation:\n",
        "            st.write(f\"### Evaluation Result for: {uploaded_file.name} (from cache)\")\n",
        "            st.markdown(existing_evaluation)\n",
        "            button_key = f\"upload_{uploaded_file.name}\"\n",
        "            if st.button(f\"Upload Evaluation to Google Drive\", key=button_key):\n",
        "                drive_uploader.upload_evaluation(uploaded_file.name, existing_evaluation)\n",
        "            continue\n",
        "\n",
        "        evaluation_result = evaluate_uploaded_thesis(uploaded_file, uploaded_description, vdb, drive_uploader, input_processor, use_rag, uploaded_criteria_files)\n",
        "\n",
        "        st.session_state['interaction_history'].append({\n",
        "            'filename': uploaded_file.name,\n",
        "            'evaluation': evaluation_result\n",
        "        })\n",
        "\n",
        "        button_key = f\"upload_{uploaded_file.name}\"  # unique key per file\n",
        "        if st.button(f\"Upload Evaluation to Google Drive\", key=button_key):\n",
        "            drive_uploader.upload_evaluation(uploaded_file.name, evaluation_result)\n",
        "\n",
        "# --- Display Selected Interaction ---\n",
        "if 'selected_interaction' in st.session_state and st.session_state['selected_interaction']:\n",
        "    st.header(f\"Previous Evaluation for: {st.session_state['selected_filename']}\")\n",
        "    st.markdown(st.session_state['selected_interaction'])\n",
        "\n",
        "    # Add the \"Upload to Google Drive\" button for the selected interaction\n",
        "    button_key = f\"upload_{st.session_state['selected_filename']}\"  # Unique button key based on the selected file name\n",
        "    if st.button(f\"Upload Evaluation to Google Drive\", key=button_key):\n",
        "        drive_uploader.upload_evaluation(st.session_state['selected_filename'], st.session_state['selected_interaction'])\n",
        "    if st.button(\"Clear Evaluation\"):\n",
        "        st.session_state['selected_interaction'] = None\n",
        "        st.session_state['selected_filename'] = None\n",
        "        st.rerun()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLRAodNweFAJ",
        "outputId": "e5b9b667-a080-4ece-80f7-e7506241544f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "pjLdhh_vb7jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"2rXKEgHaIEJUlHFu1BadyYgQ3nw_61LEhKiQ4gMDUHV9XiUYf\""
      ],
      "metadata": {
        "id": "GZo1GiKzb_Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b56d49-c1d0-46c8-cc27-5fc3a0d23cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n",
        "public_url"
      ],
      "metadata": {
        "id": "T9iH3vuOcCg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc1e79a-551c-4ae2-f99c-8096306499c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://a6ef-34-91-182-200.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhsEqYRrmmqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}